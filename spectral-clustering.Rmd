---
title: "Intro to Computer Science Data Analysis"
author: ['Jonathan Mendes de Almeida', 'jonathanalmd@gmail.com', 'jonathan@aluno.unb.br','@jonyddev (github)']
lang: en
date: "`r format(Sys.time(), '%b %d, %Y')`"
output:
    pdf_document:
      latex_engine: xelatex
---

# Useful links
* Pseudocode for k-means: http://www.devmedia.com.br/data-mining-na-pratica-algoritmo-k-means/4584

* Similar algorithm: http://www.di.fc.ul.pt/~jpn/r/spectralclustering/spectralclustering.html (uses knn plus step one and do not executes the step five)
Usa knn junto no passo 1 e nao executa passo 4 do algoritmo do artigo, meio diferente mas funciona

```{r setup, include=FALSE}
# Set Rmd file location as current directory
knitr::opts_knit$set(root.dir = getwd())
```

# PETAL LENGTH/WIDTH DATA

##Init

### Get data from file and init vars
```{r eval=TRUE}
Dados <- read.csv("iris.data.csv", header=FALSE)
precision <- 5 # float precision
my.data <- as.matrix(Dados[,c(3,4)])

n <- nrow(my.data) # n = number of columns from dataset
S <- my.data # data to S
A <- matrix(rep(0,n^2) ,nrow = n ,ncol=n) # create empty matrix
# sigma2 <- sum ((S - mean(S))^2) / (n)  #var pop
sigma2 <- 3 # set a sigma (0 to 5 are good values)
D <- diag(n) # create a diagonal matrix 
```


## Spectral Clustering Algorithm

### Step 1: Compute A Matrix (Affinity Matrix)__
```{r eval=TRUE}
for (i in 1:n){
  for(j in 1:n){
    if (i != j){
      # Euclidean distance
      A[i,j] <- exp( - sqrt(sum((S[i,]-S[j,])^2)) / 2*sigma2)
      #A[i,j] <- exp(- norm(as.matrix(S[i,]-S[j,]), type="F"))
    }
  }  
}
# set float precision (5)
round(A[1:8,1:8],precision)
```

### Step 2: Compute D Matrix
__2.1 Calcular matriz D__
```{r eval=TRUE}
for (i in 1:n){ 
  # sum of each row and insert into the diagonal matrix D
  D[i,i] <- sum (A[i,])
}
# set float precision (5)
round(D[1:8,1:8],precision)
```

### Step 3: Compute L Matrix (using D)
```{r eval=TRUE}
# get sqrt of each matrix element 
raiz.D     <- sqrt (D)       #obs1 : raiz.D %*% raiz.D = D
                                  # raiz.D x raiz.D = D (mult matrix)
# solve() para pegar inversa
# solve() function to get inverse matrix
Inv.raiz.D <- solve(raiz.D)  #obs2 : compute inverse matrix = get sqrt from inverse matrix
                             # sqrt (solve (D)) = solve (sqrt (D))  
# Compute L
L <- Inv.raiz.D %*% A %*% Inv.raiz.D

# Set float precision (5)
round(L[1:8,1:8],precision)
```

### Step 4: Compute eigenvector and set matrix X with the k first eigenvectors from L
```{r eval=TRUE}
#get eigenvector
autovet <- eigen (L)$vectors
#autoval <- eigen (L)$values 

# set the number of classes (k)
k <- 3

# get the 3 first eigenvectors (k first eigenvalues)
X <- autovet[,(1 : k)]
X
#dec.espec <- autovet %*% diag(autoval) %*% t(autovet) -> return to L matrix 
```

### Step 5: Compute Y Matrix using X Matrix: Normalize Y Matrix
```{r eval=TRUE}
# Create a matrix with n rows and k columns 
Y <- matrix (0,nrow=n,ncol=k)

# For each element from X, div(elem)/sqrt(each element from row squared)
for(i in 1:n){
  for(j in 1:k){
    Y[i,j] <- X[i,j] / (sqrt (sum(X[i,j])^2))
  }
}
# Normalized Y matrix: only 1 and -1 values
Y
```

## K-Means (Step 6)
### Step 6.1: Set vars and set centroids

```{r eval=TRUE}
xnew <- Y
obs <- as.numeric()

# seta centroides iniciais (criando combinacao de -1 e 1) -> RUIM
#center1<- sample(seq(-1,1,by=0.1),3,replace=T)
#center2<- sample(seq(-1,1,by=0.1),3,replace=T)
#center3<- sample(seq(-1,1,by=0.1),3,replace=T)

# seta centroides iniciais com heuristica (sabendo que esses 3 pontos sao disitntos)
#center1 <- xnew[1,] 
#center2 <- xnew[70,]
#center3 <- xnew[149,]

# trocar -1 por 0 para ficar tudo com valores 0 ou 1
#for(n in 1:150){
#  for(m in 1:3){
#    if (xnew[n,m] == -1){
#      xnew[n,m] = 0
#    }
#  }
#}

# get different centroids 
flag <- TRUE
while(flag){
  center1 <- xnew[sample(1:150,1),]
  center2 <- xnew[sample(1:150,1),]
  center3 <- xnew[sample(1:150,1),]
  if (!(all(center1 == center2) || all(center1 == center3) || all(center2 == center3)) ){
    icenter1 <- center1
    icenter2 <- center2
    icenter3 <- center3
    flag <- FALSE
  }
}
```

### Step 6.2: K-Means Algorithm and Plot Graphs
```{r eval=TRUE}
for(n in 1:40000){ # upgrade centroids
  for(i in 1:150){ # 150 instances
    dist1<- sum((xnew[i,]-center1)^2)
    dist2<- sum((xnew[i,]-center2)^2)
    dist3<- sum((xnew[i,]-center3)^2)
    
    if(dist1<=dist2 && dist1<=dist3){
      obs[i]<-1 
    }
    else if(dist2<=dist1 && dist2<=dist3){
      obs[i]<-2
    }  
    else{
      obs[i]<-3
    }
  }
  
  grupo1<-xnew[(obs == 1),]
  grupo2<-xnew[(obs == 2),]
  grupo3<-xnew[(obs == 3),]
  
  d1 <- dim(grupo1)[1]
  d2 <- dim(grupo2)[1]
  d3 <- dim(grupo3)[1]
  
  # Check if different class
  if (d1 != 0){
    center1<-c(mean(grupo1[,1]),mean(grupo1[,2]),mean(grupo1[,3]))
  }
  
  if (d2 != 0){
    center2<-c(mean(grupo2[,1]),mean(grupo2[,2]),mean(grupo2[,3]))
  }
  
  if (d3 != 0){
    center3<-c(mean(grupo3[,1]),mean(grupo3[,2]),mean(grupo3[,3]))
  }
  
}
# R kmeans
km <- kmeans (Y,4,nstart=20)
#km <- kmeans (X,3,nstart=20) 
plot(S, col=km$cluster)
km <- kmeans (Y,3,nstart=20)  
#km <- kmeans (X,3,nstart=20) 

# Plot both
plot(S, col=km$cluster, main = 'Petal Width vs Petal Length', sub = 'Species classification', xlab = 'Petal Width', ylab = 'Petal Length')
plot(S, col=obs, main = 'Petal Width vs Petal Length', sub = 'Species classification', xlab = 'Petal Width', ylab = 'Petal Length')

```

### Compare centroinds values (initial and final values)

```{r eval=TRUE}
center1
center2
center3

icenter1
icenter2
icenter3
```






